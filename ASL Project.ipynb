{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8da4963-7947-450d-bc50-28047492758a",
   "metadata": {},
   "source": [
    "## American Sign Language detection using Handtracking"
   ]
  },
 
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd0a8d-6ff5-49f3-ab18-7925f780a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import tkinter as tk\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the CNN model\n",
    "def create_cnn_model(input_shape=(64, 64, 3), num_classes=29):  # Example for ASL letters\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model using data from directories\n",
    "def train_model(train_dir, test_dir):\n",
    "    # Data augmentation and rescaling\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load the images from the folder path\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir, \n",
    "        target_size=(64, 64), \n",
    "        batch_size=32, \n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir, \n",
    "        target_size=(64, 64), \n",
    "        batch_size=32, \n",
    "        class_mode='categorical', \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {train_generator.samples}\")\n",
    "    print(f\"Test samples: {test_generator.samples}\")\n",
    "\n",
    "    model = create_cnn_model(input_shape=(64, 64, 3), num_classes=train_generator.num_classes)\n",
    "\n",
    "    model.fit(train_generator, epochs=2, verbose=1)\n",
    "\n",
    "    if test_generator.samples > 0:\n",
    "        test_loss, test_acc = model.evaluate(test_generator)\n",
    "        print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"No test samples available for evaluation.\")\n",
    "\n",
    "    return model, list(test_generator.class_indices.keys())\n",
    "\n",
    "# Hand detection and prediction function\n",
    "def hand_tracking(model, class_names):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    predicted_letter = \"No Hand Detected\"\n",
    "\n",
    "    with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.8) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Extract the hand landmarks (edges)\n",
    "                    hand_landmarks_array = []\n",
    "                    for lm in hand_landmarks.landmark:\n",
    "                        hand_landmarks_array.append([int(lm.x * image.shape[1]), int(lm.y * image.shape[0])])\n",
    "                    \n",
    "                    # Create edges for the hand\n",
    "                    hand_landmarks_array = np.array(hand_landmarks_array)\n",
    "                    x, y, w, h = cv2.boundingRect(hand_landmarks_array)\n",
    "                    hand_roi = image[y:y + h, x:x + w]\n",
    "\n",
    "                    # Preprocess hand ROI for the model\n",
    "                    if hand_roi.size == 0:\n",
    "                        continue\n",
    "                    hand_roi = cv2.resize(hand_roi, (64, 64)) / 255.0\n",
    "                    hand_roi = np.expand_dims(hand_roi, axis=0)\n",
    "\n",
    "                    # Predict the letter\n",
    "                    pred = model.predict(hand_roi)\n",
    "                    predicted_letter = class_names[np.argmax(pred)]\n",
    "\n",
    "            # Show the predicted letter on screen\n",
    "            cv2.putText(image, f\"Predicted: {predicted_letter}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow('Hand Tracking', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# GUI function to train and test the model\n",
    "def gui():\n",
    "    global train_dir, test_dir\n",
    "    train_dir = r\"C:\\Users\\17cha\\Downloads\\asl\\asl_alphabet_train\"\n",
    "    test_dir = r\"C:\\Users\\17cha\\Downloads\\asl\\asl_alphabet_test\"\n",
    "\n",
    "    def start_training():\n",
    "        global model, class_names\n",
    "        model, class_names = train_model(train_dir, test_dir)\n",
    "\n",
    "    def start_testing():\n",
    "        hand_tracking(model, class_names)\n",
    "\n",
    "    # Setup GUI\n",
    "    window = tk.Tk()\n",
    "    window.title(\"ASL Recognition\")\n",
    "\n",
    "    train_button = tk.Button(window, text=\"Train Model\", command=start_training)\n",
    "    train_button.pack(pady=10)\n",
    "\n",
    "    test_button = tk.Button(window, text=\"Start Testing (Camera)\", command=start_testing)\n",
    "    test_button.pack(pady=10)\n",
    "\n",
    "    window.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = None\n",
    "    class_names = None\n",
    "    gui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03876ea-2696-4875-8f7f-806bfcb24b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d76ff05-bb6f-48a6-adb6-98461e7bf31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.19.1-cp38-cp38-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: torch==2.4.1 in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from torchvision) (2.4.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from torch==2.4.1->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from torch==2.4.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from torch==2.4.1->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from torch==2.4.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from torch==2.4.1->torchvision) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from jinja2->torch==2.4.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\17cha\\anaconda3\\envs\\dlpro\\lib\\site-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.19.1-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 558.9 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 558.9 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 0.8/1.3 MB 524.3 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.8/1.3 MB 524.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 713.6 kB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.19.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f622964-48c7-4173-bda2-c26bf0fa6405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the CNN model\n",
    "class ASLCNN(nn.Module):\n",
    "    def __init__(self, num_classes=29):  # For 29 ASL letters (A-Z, Space, Delete, Nothing)\n",
    "        super(ASLCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Load dataset\n",
    "def load_data(train_dir, test_dir, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader, train_dataset.classes\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10, learning_rate=0.001):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Real-time hand tracking and prediction\n",
    "def hand_tracking(model, class_names):\n",
    "    model.eval()\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    predicted_letter = \"No Hand Detected\"\n",
    "\n",
    "    with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.8) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Extract ROI for the hand\n",
    "                    hand_landmarks_array = []\n",
    "                    for lm in hand_landmarks.landmark:\n",
    "                        hand_landmarks_array.append([int(lm.x * image.shape[1]), int(lm.y * image.shape[0])])\n",
    "                    hand_landmarks_array = np.array(hand_landmarks_array)\n",
    "                    x, y, w, h = cv2.boundingRect(hand_landmarks_array)\n",
    "                    hand_roi = image[y:y + h, x:x + w]\n",
    "\n",
    "                    # Preprocess for the model\n",
    "                    if hand_roi.size == 0:\n",
    "                        continue\n",
    "                    hand_roi = cv2.resize(hand_roi, (64, 64)) / 255.0\n",
    "                    hand_roi = np.transpose(hand_roi, (2, 0, 1))  # HWC to CHW\n",
    "                    hand_roi = torch.tensor(hand_roi, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "                    # Predict the letter\n",
    "                    pred = model(hand_roi)\n",
    "                    predicted_letter = class_names[torch.argmax(pred).item()]\n",
    "\n",
    "            # Show prediction\n",
    "            cv2.putText(image, f\"Predicted: {predicted_letter}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow('Hand Tracking', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir = r\"C:\\Users\\17cha\\Downloads\\asl\\asl_alphabet_train\"\n",
    "    test_dir = r\"C:\\Users\\17cha\\Downloads\\asl\\asl_alphabet_test\"\n",
    "\n",
    "    train_loader, test_loader, class_names = load_data(train_dir, test_dir)\n",
    "    model = ASLCNN(num_classes=len(class_names))\n",
    "    model = train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "    hand_tracking(model, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9b8bc-ff79-4b1a-8893-b3b66e5ad3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
